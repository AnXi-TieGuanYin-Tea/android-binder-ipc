
28 Jan 2010
The test program binder_test works like a charm. It timestamps a transaction-reply roundtrip time, plus all the critical points the data goes through. Example shown below:

  C_SEND	   K_IOC	 K_WR_IN	   K_ENQ	   K_DEQ	  K_COPY	  S_RECV	 S_REPLY	   K_IOC	 K_WR_IN	   K_ENQ	   K_DEQ	  K_COPY	  C_RECV	  C_EXIT	
0.000000	0.000001	0.000002	0.000004	0.000012	0.000013	0.000014	0.000016	0.000017	0.000018	0.000019	0.000028	0.000030	0.000031	0.000033	
0.000000	0.000002	0.000003	0.000004	0.000012	0.000013	0.000015	0.000016	0.000017	0.000018	0.000019	0.000028	0.000030	0.000031	0.000033	
0.000000	0.000002	0.000003	0.000004	0.000012	0.000013	0.000014	0.000016	0.000017	0.000018	0.000019	0.000028	0.000030	0.000031	0.000032	

The test shows that with a simple C implementation of server and client, a two-way transaction round trip time is about 33us, comparing to 150us-300us I have seen with binderAddInts, which was implemented op top of the framework. Obviously the framework wasn't implemented that efficiently.









!!! Tested only on x86 32bit only (for now) - I'm using Ubuntu 10.04

To Build:

1. Build binder module(s)
	$ cd module; make
	* Two drivers built: binder_new.ko the version I'm working on, and binder_old.ko the existing binder driver
	* For binder_old.ko, see gen_deps.sh & deps.c for how missing symbols are patched :) 

2. Build service manager
	$ cd servicemanager; make

3. Build binder library
	$ cd libs; make

4. Build the test program
	$ cd test; make


To Test (run as root for now)
1. Load driver
	# cd moudle; insmod binder_new.ko

2. Start service manager
	# cd servicemanager; ./servicemanager &

3. Run the test program 
	# cd test
	# ./binderAddInts -n 10000
	# ./binderAddInts -n 10000 -d 0

Notes:
1. The two drivers are currently incompatible. To run the above tests on the old driver, one needs to comment out INLINE_TRANSACTION_DATA macro in the following two files and recompile everything:
	libs/binder/IPCThreadState.cpp
	servicemanager/binder.c


2. With the old driver, run the above two tests - one with delay between iterations and the other one without - you see the average iteration delay is much longer for the one without delay. It's most likely due to inefficient locking in the binder driver, i.e. global binder_lock. So I guess it's step one for this project to address.

	# ./binderAddInts -n 10000
serverCPU:  unbound
clientCPU:  unbound
iterations: 10000
iterDelay: 0.001
Time per iteration min: 0.000151486 avg: 0.000181443 max: 0.00497661

	# ./binderAddInts -n 10000 -d 0
serverCPU:  unbound
clientCPU:  unbound
iterations: 10000
iterDelay: 0
Time per iteration min: 0.000139962 avg: 0.000258004 max: 0.00490384


3. The current results of the new driver. Not so much better, but it's just some first tests, tuning on the way
	# ./binderAddInts -n 10000
serverCPU:  unbound
clientCPU:  unbound
iterations: 10000
iterDelay: 0.001
Time per iteration min: 0.000142057 avg: 0.000203101 max: 0.00329881

	# ./binderAddInts -n 10000 -d 0
serverCPU:  unbound
clientCPU:  unbound
iterations: 10000
iterDelay: 0
Time per iteration min: 0.000139124 avg: 0.000242037 max: 0.00243285

